{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPd6xBPGHYSrXvwUZr2Xvqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viett887g/2023-10-27-baitapML/blob/main/redo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyUNVzmDWWWO",
        "outputId": "66849831-44da-4b20-ffdc-31c7c8bce21b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "0M9aTuolVmaX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I. Chọn dataset :USA Housing**"
      ],
      "metadata": {
        "id": "s6yZtcBZoKx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm đọc dữ liệu từ file\n",
        "def readdata(file):\n",
        "    data = pd.read_csv(file)\n",
        "    del data['Address']\n",
        "    data = np.array(data)\n",
        "    return data\n",
        "\n",
        "# Hàm chuẩn hóa dữ liệu và tạo training set và test set\n",
        "def create_data(data):\n",
        "    data = preprocessing.MinMaxScaler().fit_transform(data)\n",
        "    X = data[:,:-1]\n",
        "    y = data[:, -1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Hàm sigmoid\n",
        "def sigmoid(X, w):\n",
        "    Hw = 1 / (1 + np.exp(-np.dot(X, w)))\n",
        "    return Hw\n",
        "\n",
        "# Hàm tính giá trị los\n",
        "def loss(w, X, y):\n",
        "    m = X.shape[0]\n",
        "    h = sigmoid(X, w)\n",
        "    result = (-1 / m) * np.sum(np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "    return result"
      ],
      "metadata": {
        "id": "L2ftg5TSbHLM"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Chọn 2 phương pháp **Gradient Descent (GD)**, **Momentum Based GD** tối ưu để tối ưu hoá loss function Linear Regression cho tập dữ liệu USA Housing"
      ],
      "metadata": {
        "id": "2ckxkM8DoQbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.1 Gradient Descent (GD)**"
      ],
      "metadata": {
        "id": "3RR9UEa_pN3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm gradient descent\n",
        "def gradient_descent(X, y, w, learning_rate, num_iterations):\n",
        "    m = X.shape[0]\n",
        "    loss_history = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = sigmoid(X, w)\n",
        "        gradient = (1 / m) * np.dot(X.T, (h - y))\n",
        "        w = w - learning_rate * gradient\n",
        "        loss_value = loss(w, X, y)\n",
        "        loss_history.append(loss_value)\n",
        "\n",
        "    return w, loss_history"
      ],
      "metadata": {
        "id": "OG9lRlKdgvWv"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Momentum Based GD**"
      ],
      "metadata": {
        "id": "F9JNfaccpXbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_based_gradient_descent(X, y, w, learning_rate, num_iterations, momentum):\n",
        "    m = X.shape[0]\n",
        "    loss_history = []\n",
        "    velocity = np.zeros_like(w)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        h = sigmoid(X, w)\n",
        "        gradient = (1 / m) * np.dot(X.T, (h - y))\n",
        "\n",
        "        velocity = momentum * velocity + learning_rate * gradient\n",
        "        w = w - velocity\n",
        "\n",
        "        loss_value = loss(w, X, y)\n",
        "        loss_history.append(loss_value)\n",
        "\n",
        "    return w, loss_history"
      ],
      "metadata": {
        "id": "LjUTjXWkhnUA"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **III. So sánh hiệu suất của các thuật toán**"
      ],
      "metadata": {
        "id": "d3XE99OipcAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(X_test, y_test, w):\n",
        "    # Tính toán dự đoán trên tập test\n",
        "    y_pred = np.dot(X_test, w)\n",
        "\n",
        "    # Tính toán Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Tính toán Rmse\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "\n",
        "    # Trả về kết quả hiệu suất của mô hình\n",
        "    return mse, rmse"
      ],
      "metadata": {
        "id": "VjeaL1WHkU87"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    data = readdata(\"/content/drive/MyDrive/USA_Housing.csv\")\n",
        "    X_train, X_test, y_train, y_test = create_data(data)\n",
        "    # Khởi tạo vector trọng số w\n",
        "    w = np.zeros(X_train.shape[1])\n",
        "    # Thiết lập tỷ lệ học tập và số lần lặp\n",
        "    learning_rate = 0.01\n",
        "    num_iterations = 1000\n",
        "    momentum = 0.9\n",
        "    # Thực hiện gradient descent để tối ưu hàm loss và trả về vector trọng số tối ưu và lịch sử giá trị loss\n",
        "    w_optimal, loss_history = gradient_descent(X_train, y_train, w, learning_rate, num_iterations)\n",
        "\n",
        "    # Tính giá trị loss tối ưu trên tập test\n",
        "    optimal_loss = loss(w_optimal, X_test, y_test)\n",
        "    print(\"Giá trị loss tối ưu của Gradient Descents :\", optimal_loss)\n",
        "    mse, rmse = evaluate_performance(X_test, y_test, w_optimal)\n",
        "\n",
        "    print(\"Mean Squared Error của Gradient Descents (MSE):\", mse)\n",
        "    print('Root Mean Squared Error của Gradient Descents:',rmse)\n",
        "    print('\\n','\\n')\n",
        "\n",
        "    w_optimal, loss_history = momentum_based_gradient_descent(X_train, y_train, w, learning_rate, num_iterations, momentum)\n",
        "\n",
        "    optimal_loss = loss(w_optimal, X_test, y_test)\n",
        "    print(\"Giá trị loss tối ưu của momentum_based_gradient_descent :\", optimal_loss)\n",
        "    mse, rmse = evaluate_performance(X_test, y_test, w_optimal)\n",
        "\n",
        "    print(\"Mean Squared Error (MSE) của momentum_based_gradient_descent:\", mse)\n",
        "    print('Root Mean Squared Error của momentum_based_gradient_descent:',rmse)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQJPj5wyftYg",
        "outputId": "fcfb6967-6320-44ed-b377-159d68addf02"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Giá trị loss tối ưu của Gradient Descents : 0.6926051747191877\n",
            "Mean Squared Error của Gradient Descents (MSE): 0.22673429024886363\n",
            "Root Mean Squared Error của Gradient Descents: 0.47616624223989634\n",
            "\n",
            " \n",
            "\n",
            "Giá trị loss tối ưu của momentum_based_gradient_descent : 0.692207168506447\n",
            "Mean Squared Error (MSE) của momentum_based_gradient_descent: 0.22426197010903098\n",
            "Root Mean Squared Error của momentum_based_gradient_descent: 0.4735630582182599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IV. Nhận xét**\n",
        "\n",
        "mô hình momentum_based_gradient_descent có giá trị loss tối ưu thấp hơn, cho thấy mô hình này có hiệu suất tốt hơn trong việc tối ưu hóa hàm mất mát.\n",
        "\n",
        "mô hình momentum_based_gradient_descent có giá trị MSE thấp hơn, cho thấy mô hình này có khả năng dự đoán chính xác hơn và đạt được mức độ biến động giữa giá trị dự đoán và giá trị thực tế tốt hơn.\n",
        "\n",
        "mô hình momentum_based_gradient_descent có giá trị RMSE thấp hơn, cho thấy mô hình này có khả năng dự đoán chính xác hơn và có độ lệch trung bình giữa giá trị dự đoán và giá trị thực tế thấp hơn."
      ],
      "metadata": {
        "id": "l7npH17UnqP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IV. Kết luận**\n",
        "Dựa trên các kết quả trên, mô hình momentum_based_gradient_descent có giá trị loss thấp hơn và các chỉ số MSE và RMSE nhỏ hơn, do đó có thể xem là mô hình này tối ưu hơn và tốt hơn so với mô hình Gradient Descents."
      ],
      "metadata": {
        "id": "jZaX48GlqWC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V. Ưu nhược điểm của phương pháp tối ưu.**"
      ],
      "metadata": {
        "id": "lx9pMhbjrunj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm của phương pháp tối ưu Gradient Descent (GD):**\n",
        "\n",
        "Đơn giản và dễ hiểu: Gradient Descent là một phương pháp tối ưu đơn giản và dễ hiểu. Nó chỉ yêu cầu tính đạo hàm đơn giản của hàm mất mát, và cập nhật các tham số theo hướng giảm độ dốc của đạo hàm.\n",
        "\n",
        "Hiệu quả tính toán: GD thường được sử dụng trong nhiều mô hình học máy công nghệ cao, bởi vì nó có thể xử lý tập dữ liệu lớn một cách hiệu quả. Việc tính toán gradient chỉ phụ thuộc vào kích thước của batch, và không yêu cầu tính toán ma trận đầy đủ.\n",
        "\n",
        "Hội tụ tới điểm tối ưu cục bộ: Nếu được cài đặt đúng, GD có khả năng hội tụ tới điểm tối ưu cục bộ. Nếu hàm mất mát là lồi và tốt hơn, GD có thể cung cấp giải pháp tối ưu đối tượng.\n",
        "\n",
        "**Nhược điểm của phương pháp tối ưu Gradient Descent (GD):**\n",
        "\n",
        "Tốc độ hội tụ chậm: GD có thể hội tụ chậm đối với một số bài toán, đặc biệt là khi hàm mất mát có đặc trưng vùng trong lỏng lẻo hoặc bề mặt lõm. Việc cập nhật các tham số trên toàn bộ dữ liệu có thể cần nhiều vòng lặp để đạt điểm tối ưu.\n",
        "Momentum Based Gradient Descent (GD với đà):\n"
      ],
      "metadata": {
        "id": "FuW_xEWssFpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm Momentum Based GD** có thể giúp cải thiện tốc độ hội tụ của GD. Bằng cách tích lũy đà từ các bước trước, nó có khả năng vượt qua các mục tiêu hẹp và giảm thiểu trỗi dạng cục bộ, đồng thời giảm ảnh hưởng của các điểm nhiễu trên đường đi hội tụ.\n",
        "\n",
        "**Nhược điểm Momentum Based GD**: Mặc dù hiệu quả hơn so với GD cơ bản, tuy nhiên không phải lúc nào Momentum Based GD cũng tốt hơn. Trong một số trường hợp, điều này có thể gây ra quá đà, kéo dài thời gian hội tụ hoặc không hội tụ tới điểm tối ưu. Bên cạnh đó, việc điều chỉnh thông số đà có thể không dễ dàng và yêu cầu thử nghiệm."
      ],
      "metadata": {
        "id": "QrEJ5YEasdyM"
      }
    }
  ]
}